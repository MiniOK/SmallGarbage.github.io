### 多进程（multiprocessing）

- 多进程可以加快文件处理速度
- 多进程充分利用电脑资源，将全部核心跑满
- 多进程还很好实现

个人理解：多进程就是将数据分批分块处理

```python
import multiprocessing as mp         # 导入多线程包
```

定义一个work方法，让这个方法实现复杂的处理过程，我们的多进程只是将数据分块扔进这个work方法中

```python
def worker(pid,file):
    print("========== Process{} has started =========".format(pid))
    # 这中间写我们处理文件的过程方法，自定义
    print("========== Process{} has finished =========".format(pid))
```

下面配置我们的多进程部分

```python
if __name__ == "__main__":
    # 初始化一个Manager对象
    manager = mp.Manager()
    # 找到我们要处理数据的长度
    num = len(file_list)
    # 定义进程数，根据进程与数据量计算我们每一块的数量
    process_num = 8       # 备选 电脑是8核就是8  16核就是16
    inputs = []           # 相当于每个进程
    chunk = int(num/process_num)
    start_ind = 0         # 分割数据记数
    # 分配数据到每个进程
    for i in range(process_num):
        inputs.append(file_list(start_ind:start_ind + chunk))
        start_ind += chunk +1
    # 判断数据是否瓜分完全，将剩余数据塞进最后一个进程中    
    if chunk * process_num < num:
        num_1 = 0 - num - process_num * chunk
        inputs[-1] = inputs[-1] + fl[num_1:]
    # 定义一个计数器，已经存放进程的数组
    counter = 0
    processes = []
    # 定义这些进程，并存入数据
    # 定义mp.Process对象，target为目标方法，args为输入参数
    for input in inputs:
        processes.append(mp.Process(target=worker, args=(counter, input)))
        counter += 1
    # 运行所有进程
    for p in processes:
        p.start()
    # 确定所有进程结束
    for p in processes:
        p.join()
        
```

简单的一个小多进程就完事了QAQ

发现太短了！ 让我们举个栗子吧===

举一个画心电图的栗子，说很久以前，我们有一堆数据，这个数据呢就是病人的心电图数据，我们要根据这个心电信号画心电图，画图的方法我们可以写一个，在利用多进程，事半功倍说的就是这个过程，下面上代码

```python
import os
import scipy.io
import numpy as np
import matplotlib.pyplot as plt
import multiprocessing as mp


def worker(pid, file_list, tORt):
    """
    This method is used for ECG data visualization
    It will read though all files in file_list and output ecg image with coordinates to output folder
    :param pid: process ID
    :param file_list: list of file names for this process
    :param tORt: test or train
    :return: Nothing
    """
    print("===================Process {} has started===================".format(pid))
    labels = ["I", "II", "III", "avr", "avl", "avf", "V1", "V2", "V3", "V4", "V5", "V6"]
    fig = plt.figure(figsize=(60, 5))   # 先弄一张大画布

    for i in range(len(file_list)):
        fn = file_list[i]
        mat = scipy.io.loadmat(dirr + fn)
        print("Reading file {} ... ".format(fn))
        mat_data = mat["data"]         # 加载心电数据

        os.mkdir("../../../output/{}/{}".format(tORt, fn[:-4])) # 为每个图片建文件夹

        for j in range(len(mat_data)):
            label = labels[j]

            wave = mat_data[j]

            n = 1
            wave = [float(wave[i]) for i in range(len(wave))]  # 心电信号数据
            freq = 500
            Time = np.linspace(0, int(len(wave)) / freq, num=int(len(wave))) # 构建时间序列

            plt.title("{}_{}".format(fn[:-4], label)) # 为每个图片赋予名字
            ax = fig.add_subplot(1, 1, 1)

            # Major ticks every 20, minor ticks every 5
            major_xticks = np.arange(0, 12, 0.2)
            minor_xticks = np.arange(0, 12, 0.04)

            major_yticks = np.arange(min(wave) - 0.1, max(wave) + 0.1, step=0.5)
            minor_yticks = np.arange(min(wave) - 0.1, max(wave) + 0.1, step=0.1)

            ax.set_xticks(major_xticks)
            ax.set_xticks(minor_xticks, minor=True)
            ax.set_yticks(major_yticks)
            ax.set_yticks(minor_yticks, minor=True)

            # And a corresponding grid
            ax.grid(which='both')

            # Or if you want different settings for the grids:
            ax.grid(which='minor', alpha=0.3)
            ax.grid(which='major', alpha=0.7)

            plt.plot(Time, wave)
            # plt.xticks(np.arange(0, 12, 0.04))
            # plt.yticks(np.arange(min(wave) - 0.1, max(wave) + 0.1, step=0.1))
            plt.grid(color="r")
            plt.gca().set_aspect(1 / 2.5)

            fig_name = "{}_{}.png".format(fn[:-4], label)
            print("Saving {} with {} to {} ...".format(fn, label, fig_name))
            # plt.show()
            plt.savefig("../../../output/{}/{}/{}".format(tORt, fn[:-4], fig_name))
            plt.clf()
    print("===================Process {} has Finished===================".format(pid))


if __name__ == "__main__":

    # Train or Test
    which = "TEST"

    # 根目录路径
    dirr = "../../../data/preliminary/{}/".format(which)
    fl = os.listdir(dirr)

    # 初始化一个Manager对象
    manager = mp.Manager()

    # 定义数据并找到其长度
    num = len(fl)

    # 定义进程数，并由进程数与数据量算出每一块数据的数量
    process_num = 16
    inputs = []
    chunk = int(num / process_num)
    start_ind = 0

    # 预配置所有worker方法的输入
    for _ in range(process_num):
        inputs.append(fl[start_ind: start_ind + chunk])
        start_ind += chunk

    if process_num * chunk < num:
        inputs[-1] = inputs[-1] + fl[-8:]

    # 定义一个计数器以及存放进程的数组
    counter = 0
    processes = []

    # 定义这些进程并存入数组
    for input in inputs:
        # 定义mp.Process对象，target为目标方法，args为输入参数（tuple）
        processes.append(mp.Process(target=worker, args=(counter, input, which,)))
        counter += 1

    # 运行所有进程
    for p in processes:
        p.start()

    # 确定所有进程结束
    for p in processes:
        p.join()

```

