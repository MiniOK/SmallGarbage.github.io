### 多进程（multiprocessing）

- 多进程可以加快文件处理速度
- 多进程充分利用电脑资源，将全部核心跑满
- 多进程还很好实现

个人理解：多进程就是将数据分批分块处理

```python
import multiprocessing as mp         # 导入多线程包
```

定义一个work方法，让这个方法实现复杂的处理过程，我们的多进程只是将数据分块扔进这个work方法中

```python
def worker(pid,file):
    print("========== Process{} has started =========".format(pid))
    # 这中间写我们处理文件的过程方法，自定义
    print("========== Process{} has finished =========".format(pid))
```

下面配置我们的多进程部分

```python
if __name__ == "__main__":
    # 初始化一个Manager对象
    manager = mp.Manager()
    # 找到我们要处理数据的长度
    num = len(file_list)
    # 定义进程数，根据进程与数据量计算我们每一块的数量
    process_num = 8       # 备选 电脑是8核就是8  16核就是16
    inputs = []           # 相当于每个进程
    chunk = int(num/process_num)
    start_ind = 0         # 分割数据记数
    # 分配数据到每个进程
    for i in range(process_num):
        inputs.append(file_list(start_ind:start_ind + chunk))
        start_ind += chunk +1
    # 判断数据是否瓜分完全，将剩余数据塞进最后一个进程中    
    if chunk * process_num < num:
        num_1 = 0 - num - process_num * chunk
        inputs[-1] = inputs[-1] + fl[num_1:]
    # 定义一个计数器，已经存放进程的数组
    counter = 0
    processes = []
    # 定义这些进程，并存入数据
    # 定义mp.Process对象，target为目标方法，args为输入参数
    for input in inputs:
        processes.append(mp.Process(target=worker, args=(counter, input)))
        counter += 1
    # 运行所有进程
    for p in processes:
        p.start()
    # 确定所有进程结束
    for p in processes:
        p.join()
        
```

简单的一个小多进程就完事了QAQ